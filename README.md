# User Agent Parser Comparison

This application was put together to assist us in making a decision on which UserAgent parsing library to use, and then modified some to be a bit more user friendly to encourage use among the different parsing library maintainers.  It's still a bit work in progress, but in its current state it's very useful at benchmarking the different parsing libraries (across different languages) and comparing the results of the parsers against "known" useragents (right now these come from the parser libraries' unit tests, but a known list could be built and tested against).

This is loosely based on a benchmark utility that has been around for a while (but not currently maintained): https://github.com/kenjis/user-agent-parser-benchmarks

Right now this application is entirely a CLI application. Most of the data generated by the application is stored as JSON files, so it's certainly possible that a Web UI could be built around the data files which might make analysis easier.

The CLI is written in PHP (using the Symfony Console component), but the parsers (and tests) can be written in any language. Currently, most of the parsers are written in PHP, but the structure of the application (and how it calls the parsers) was intentionally built to support parsers from any language (PRs welcome!).

## Latest "Accuracy" Results

Parser        | Version          | Browser Results    | Platform Results   | Device Results      | Accuracy Score
-----|-----|-----|-----|-----|-----
browscap-js-1           | 1.8.6020       | 37179/46982 79.13% | 44195/46982 94.07% | 23224/46982 49.43% | 120935/185584 65.16%
browscap-php-2          | 2.1.1-6021     | 37250/46982 79.29% | 44218/46982 94.12% | 23224/46982 49.43% | 121144/185584 65.28%
browscap-php-3-full     | 3.0.0-6021     | 37190/46982 79.16% | 44204/46982 94.09% | 23224/46982 49.43% | 120954/185584 65.17%
browscap-php-3-lite     | 3.0.0-6021     | 23340/46982 49.68% | 32759/46982 69.73% | 38214/46982 81.34% | 61070/129171 47.28%
browscap-php-3-standard | 3.0.0-6021     | 35965/46982 76.55% | 33952/46982 72.27% | 42079/46982 89.56% | 92077/129171 71.28%
crossjoin-1             | 1.0.5-6021     | 37193/46982 79.16% | 44204/46982 94.09% | 23225/46982 49.43% | 120958/185584 65.18%
crossjoin-2             | 2.0.0-6021     | 37108/46982 78.98% | 44217/46982 94.11% | 23055/46982 49.07% | 120474/185584 64.92%
crossjoin-3             | 3.0.0-6021     | 37108/46982 78.98% | 44217/46982 94.11% | 23055/46982 49.07% | 120474/185584 64.92%
php-get-browser         | 7.0.15-6021    | 37249/46982 79.28% | 44217/46982 94.11% | 23226/46982 49.44% | 121141/185584 65.28%
piwik-device-detector-3 | 3.7.5          | 39179/46982 83.39% | 43358/46982 92.29% | 33262/46982 70.8%  | 149405/185584 80.51%
ua-parser-php-3         | v3.4.6-18d5a22 | 39306/46982 83.66% | 35735/46982 76.06% | 36238/46982 77.13% | 116599/154576 75.43%
whichbrowser-2          | v2.0.24        | 40120/46982 85.39% | 44942/46982 95.66% | 30277/46982 64.44% | 152981/185584 82.43%
woothee-php-1           | 1.5.0          | 31778/46982 67.64% | 42491/46982 90.44% | 40298/46982 85.77% | 72550/113087 64.15%
zsxsoft-php-1           | 1.4            | 36627/46982 77.96% | 41942/46982 89.27% | 24881/46982 52.96% | 98143/154576 63.49%

[View Full Results](https://github.com/diablomedia/useragent-parser-comparison/wiki/fullresults)

## Latest Benchmark Results

Performed with the following command:

`./bin/console benchmark ./files/ua-list-all.txt --iterations=10`

### VMWare ESX Virtual Server

2.33 GHz Intel Xeon E5410, 4 GB RAM, Magnetic Disk Drive (RAID), PHP 5.6

Parser                  | Average Init Time | Average Parse Time | Average Memory Used
-----|-----|-----|-----
browscap-js-1           | 0.054s            | 13.971s            | 99.96 M
browscap-php-2          | 1s                | 20.038s            | 122.76 M
browscap-php-3-full     | 0.025s            | 6.69s              | 3.61 M
browscap-php-3-lite     | 0.021s            | 1.819s             | 1.7 M
browscap-php-3-standard | 0.028s            | 3.763s             | 2.39 M
crossjoin-1             | 0.017s            | 5.766s             | 907.94 K
crossjoin-2             | 0.038s            | 10.213s            | 1.15 M
piwik-device-detector-3 | 0.886s            | 4.087s             | 4.35 M
ua-parser-php-3         | 0.044s            | 1.742s             | 1.51 M
whichbrowser-2          | 0.107s            | 2.728s             | 20.26 M
woothee-php-1           | 0.025s            | 0.062s             | 1.04 M

### MacBook Pro

2.9 GHz Intel Core i7, 16 GB RAM, SSD Drive, PHP 7.0.15

Parser                  | Average Init Time | Average Parse Time | Average Extra Time | Average Memory Used
-----|-----|-----|-----|-----
browscap-js-1           | 0.017s            | 5.921s             | 0.084s             | 91.27 M
browscap-php-2          | 0.412s            | 2.05s              | 0.036s             | 130.89 M
browscap-php-3-full     | 0.007s            | 1.587s             | 0.026s             | 3.38 M
browscap-php-3-lite     | 0.007s            | 0.297s             | 0.021s             | 1.14 M
browscap-php-3-standard | 0.007s            | 0.741s             | 0.025s             | 2 M
crossjoin-1             | 0.003s            | 1.444s             | 0.028s             | 819.19 K
crossjoin-2             | 0.007s            | 2.411s             | 0.021s             | 948.94 K
crossjoin-3             | 0.008s            | 2.375s             | 0.021s             | 926.97 K
php-get-browser         | 0.003s            | 5.246s             | 0.929s             | 391.72 K
piwik-device-detector-3 | 0.064s            | 0.319s             | 0.023s             | 2.83 M
ua-parser-php-3         | 0.022s            | 0.178s             | 0.022s             | 1.44 M
whichbrowser-2          | 0.022s            | 0.198s             | 0.024s             | 15.2 M
woothee-php-1           | 0.005s            | 0.004s             | 0.02s              | 742.8 K
zsxsoft-php-1           | 0.004s            | 0.027s             | 0.021s             | 931.92 K


## How To Use

Clone this repo, then install composer dependencies (if composer isn't installed on your system, reference http://getcomposer.org):

`composer install`

Prepare the parsers and test suites:

`./bin/prepare`

This will initialize each of the parsers and test suites. This may require your system to have certain commands accessible on the path, like `composer` or `npm`.  This process will need to be improved to be more portable.

### Run a comparison:

`./bin/console compare`

This will prompt you to select which Test Suite to use, as well as which parser you want to test against (multiples of both can be selected). After the tests are performed, a normalization step takes place where the values of the different properties will be normalized to make comparison more relevant (special characters and spaces replaced, lower-cased, various mappings performed). After that, the results can be analyzed.

### Perform a benchmark:

`./bin/console benchmark path/to/file --iterations=5`

This will prompt you to select which parsers to benchmark, then run the parsers against the specified file (needs to contain the useragents to parse, separated by new lines). After the benchmark is performed, a table like the one above will be output showing the time taken for each parser, as well as the memory consumed (reported by the parser script itself).

### Parse some useragents:

`./bin/console parse path/to/file`

Parses the user-agents in the specified file and outputs a table showing the agent and the parsed properties. Can dump this output to CSV format.

## How Parsers are Compared

### Accuracy

Every parser extracts different pieces of data and formats/names the data differently, so comparing the output of one parser to another isn't an easy thing to do. Obviously you should pick a parser that extracts the data that **you** need, not the one that scores the best here.  That said, we did want a way to compare the relative accuracy of a parser when it's run against useragents for which the properties are known.  In order to do this across different parsers of varying "complete-ness" we selected just a handful of properties that are shared across most parsers we've looked at so far (Browser Name, Browser Version, Platform/OS Name, Platform/OS Version, Device Name, Device Brand, Device Type, Device "Is Mobile?" Flag).  This list is subject to change, and perhaps even something that we could make configurable for any given comparison.

Each of the property values are then normalized, mainly by lowercasing the value and stripping out special characters and spaces ("google-bot" and "Google Bot" would become just "googlebot"). Unfortunately that's not enough, as parsers will name some of the common values differently, so there's a mapping stage of normalization. This handles cases like "IE" and "Internet Explorer" referencing the same value.  This is done at the "data source" level, meaning, where the parser gets its useragent data from (some parsers use a general data source, like the data from the Browscap project, others use their own).  While this handles a lot of data mismatches, it doesn't deal with all of it, especially when we look at Device Name parsing.

For this reason, the report you see above was broken into different sections, showing how well a parser interpreted the Browser, Platform/OS and Device values.  Typically the Device section is the lowest accuracy for a parser (except against its own test suite) due to the inconsistencies in naming devices and because it contains four properties (that must match) instead of just two.

#### The "Accuracy Score"

To account for parsers (and test suites) that don't contain certain pieces of data, we award a "point" to a parser when it correctly identifies a property, but don't penalize the parser if it (as a whole) doesn't provide a property that the test suite it is run against does.  For example, the Woothee parser doesn't contain any device data except for the "type", so for the Device section it's only scored on whether or not it parsed the "type" property correctly. On the flip side of that, parsers aren't penalized for providing data that a test suite doesn't test for.

### Benchmarking

The benchmark command doesn't do anything special other than run a parser against a list of useragents a specified number of times. The timing information is collected by the script that runs the parser and the benchmark script, just in case there is a lot of time spent in application startup that can't be measured by the parser script itself (an example of this is PHP's native `get_browser` function. When PHP is configured to load a browscap.ini file, it reads the entire file at PHP startup, which can't be measured from within a PHP script). We've broken out the times so it's easy to tell where time is spent for any given parser. This includes an "initialization time" cost and the actual "parse time" for each useragent parsed. Any other costs that the runner script imposes that aren't measured as part of the other two times are measured in the "extra time" column.

## The Parsers

The parsers can be any useragent parser from any language (web API based parsers are possible, though rate limits and network traffic should be considered for some of the larger test suites). All this application needs from a parser is an executable script named "parse" that accepts a filename as the first parameter (which contains the useragents to parse, one on each line of the file) and returns the parsed data in the following JSON format:

```json
{
    "results": [
        {
            "useragent": "Mozilla/5.0 (iPad; U; CPU OS 3_2_1 like Mac OS X; en-us) AppleWebKit/531.21.10 (KHTML, like Gecko) Mobile/7B405",
            "parsed": {
                "browser": {
                    "name": "Mobile Safari UIWebView",
                    "version": "0.0"
                },
                "platform": {
                    "name": "iOS",
                    "version": "unknown"
                },
                "device": {
                    "name": "iPad",
                    "brand": "Apple Inc",
                    "type": "Tablet",
                    "ismobile": "true"
                }
            },
            "time": 0.003
        }
    ],
    "parse_time": 0.105,
    "init_time": 0.003,
    "memory_used": 3456745
}
```

Currently this is the list of parsers included:

 * Browscap JS 1.x (https://github.com/mimmi20/browscap-js)
 * Browscap PHP 2.x (https://github.com/browscap/browscap-php/tree/2.x)
 * Browscap PHP 3.x (Full, Standard and Lite) (https://github.com/browscap/browscap-php)
 * Crossjoin Browscap 1.x (https://github.com/crossjoin/Browscap/tree/1.x)
 * Crossjoin Browscap 2.x (https://github.com/crossjoin/Browscap/tree/2.x)
 * Crossjoin Browscap 3.x (PHP7 only) (https://github.com/crossjoin/Browscap/tree/3.x)
 * PHP's Native `get_browser` (https://secure.php.net/get_browser) - It is **strongly** recommended that this parser isn't used for large useragent lists unless you're on PHP **7.0.15/7.1.1 or later**. It is **much** too slow otherwise.
 * Piwik Device Detector 3.x (https://github.com/piwik/device-detector)
 * UA Parser PHP 3.x (https://github.com/ua-parser/uap-php)
 * WhichBrowser 2.x (https://github.com/WhichBrowser/Parser)
 * Woothee PHP 1.x (https://github.com/woothee/woothee-php)
 * ZSXSoft PHP-UserAgent 1.x (https://github.com/zsxsoft/php-useragent)

## The Test Suites

Like the parsers, there is no language requirement for a test suite to be included here.  The only requirement is that an executable script named `build` can be called and returns JSON in the following format:

```json
{
    "Mozilla/5.0 (iPad; U; CPU OS 3_2_1 like Mac OS X; en-us) AppleWebKit/531.21.10 (KHTML, like Gecko) Mobile/7B405": {
        "browser": {
            "name": "Mobile Safari UIWebView",
            "version": "0.0"
        },
        "platform": {
            "name": "iOS",
            "version": "unknown"
        },
        "device": {
            "name": "iPad",
            "brand": "Apple Inc",
            "type": "Tablet",
            "ismobile": "true"
        }
    }
}
```

Due to the nature of this format, it is a requirement that any given unique useragent has only one set of parsed fields. If a test suite contains different sets of values for a single useragent (across different files, or because other headers are expected to be present), then these either need to be excluded or merged together.

These are the test suites that are currently included:

 * Browscap (https://github.com/browscap/browscap)
 * Piwik Device Detector (https://github.com/piwik/device-detector)
 * UA Parser (https://github.com/ua-parser/uap-core)
 * WhichBrowser (https://github.com/WhichBrowser/Parser)
 * Woothee (https://github.com/woothee/woothee)
 * ZSXSoft (https://github.com/zsxsoft/php-useragent)
